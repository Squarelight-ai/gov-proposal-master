from openai import OpenAI
import streamlit as st
import anthropic
import PyPDF2
import io
from pathlib import Path
import os
from dotenv import load_dotenv
import re

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

st.set_page_config(layout="wide")

def read_pdf(uploaded_file):
    pdf_reader = PyPDF2.PdfReader(io.BytesIO(uploaded_file.getvalue()))
    text = ""
    for page in pdf_reader.pages:
        text += page.extract_text()
    return text

def split_by_headers(text):
    # ë§ˆí¬ë‹¤ìš´ í—¤ë”ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë¶„í• 
    sections = []
    current_section = ""
    next_section = ""
    
    lines = text.split('\n')
    for i, line in enumerate(lines):
        if line.startswith('#'):
            # í˜„ì¬ ì„¹ì…˜ì´ ìˆê³ , ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
            if current_section and len(current_section.strip().split('\n')) > 1:
                sections.append(current_section.strip())
            elif current_section: # ì œëª©ë§Œ ìˆëŠ” ê²½ìš°
                next_section = current_section
            current_section = next_section + line + '\n'
            next_section = ""
        else:
            current_section += line + '\n'
            
    if current_section:
        sections.append(current_section.strip())
        
    # ì„¹ì…˜ì´ ì—†ëŠ” ê²½ìš° ì¼ë°˜ ì²­í¬ë¡œ ë¶„í• 
    if not sections:
        return split_into_chunks(text)
        
    return sections

def split_into_chunks(text, chunk_size=2000):
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

with st.sidebar:
    api_provider = st.selectbox("API ì œê³µì", ["Claude", "OpenAI"])
    
    if api_provider == "OpenAI":
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            openai_api_key = st.text_input("OpenAI API í‚¤", key="openai_api_key", type="password")
        else:
            with st.expander("OpenAI API í‚¤", expanded=False):
                st.text_input("ì‚¬ì „ ì •ì˜ëœ API Keyê°’ ì…ë ¥ì™„ë£Œ", value=openai_api_key, type="password", disabled=True)
        "[OpenAI API í‚¤ ë°œê¸‰ë°›ê¸°](https://platform.openai.com/account/api-keys)"
        if openai_api_key:
            try:
                client = OpenAI(api_key=openai_api_key)
                models = client.models.list()
                model_names = [model.id for model in models.data if "gpt" in model.id]
                selected_model = st.selectbox("ëª¨ë¸ ì„ íƒ", model_names)
            except:
                st.error("ìœ íš¨í•˜ì§€ ì•Šì€ OpenAI API í‚¤ì…ë‹ˆë‹¤")
                selected_model = None
    else:
        claude_api_key = os.getenv("ANTHROPIC_API_KEY")
        if not claude_api_key:
            claude_api_key = st.text_input("Claude API í‚¤", key="claude_api_key", type="password")
        else:
            with st.expander("Claude API í‚¤", expanded=False):
                st.text_input("ì‚¬ì „ ì •ì˜ëœ API Keyê°’ ì…ë ¥ì™„ë£Œ", value=claude_api_key, type="password", disabled=True)
        "[Claude API í‚¤ ë°œê¸‰ë°›ê¸°](https://console.anthropic.com/)"
        if claude_api_key:
            try:
                claude = anthropic.Anthropic(api_key=claude_api_key)
                models = claude.models.list()
                model_names = [model.id for model in models.data]
                selected_model = st.selectbox("ëª¨ë¸ ì„ íƒ", model_names)
            except:
                st.error("ìœ íš¨í•˜ì§€ ì•Šì€ Claude API í‚¤ì…ë‹ˆë‹¤")
                selected_model = None

st.title("ğŸ’¼ ì •ë¶€ì§€ì›ì‚¬ì—… ì‚¬ì—…ê³„íšì„œ ë§ˆìŠ¤í„°")
st.caption("ğŸš€ ìƒì„±AI ê¸°ë°˜ ì‚¬ì—…ê³„íšì„œ ìë™ ì‘ì„± ì‹œìŠ¤í…œ")

# í”„ë¡¬í”„íŠ¸ ì„¤ì • (ê¸°ë³¸ê°’ ì ‘íŒ ìƒíƒœ)
with st.expander("í”„ë¡¬í”„íŠ¸ ì„¤ì •", expanded=False):
    default_prompt = "ë‹¹ì‹ ì€ ì •ë¶€ì§€ì› ì‚¬ì—… ì‚¬ì—…ê³„íšì„œë¥¼ ì‘ì„±í•˜ëŠ” ìˆ™ë ¨ëœ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê³µê³ ì™€ íšŒì‚¬ ì†Œê°œë¥¼ í†µí•´ ì ì ˆí•œ ì‚¬ì—… ê³„íšì„œë¥¼ ì¶©ì‹¤íˆ ì‘ì„±í•´ì£¼ì„¸ìš”. ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ '#', '##', '###' í—¤ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”ëœ ë¬¸ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”."
    custom_prompt = st.text_area("í”„ë¡¬í”„íŠ¸ ìˆ˜ì •", value=default_prompt, height=100)

# PDF ì—…ë¡œë“œ
uploaded_file = st.file_uploader("ê³µê³  PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”", type="pdf")

# íšŒì‚¬ ì •ë³´ ì…ë ¥
company_info = st.text_area("íšŒì‚¬ ì •ë³´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”", height=200)

if "proposal_chunks" not in st.session_state:
    st.session_state.proposal_chunks = []

if uploaded_file and company_info:
    if st.button("ì‚¬ì—…ê³„íšì„œ ìƒì„±"):
        with st.spinner("PDF ë¶„ì„ ì¤‘..."):
            pdf_text = read_pdf(uploaded_file)
            
        with st.spinner("ì‚¬ì—…ê³„íšì„œ ì‘ì„± ì¤‘..."):
            placeholder = st.empty()
            generated_text = ""
            
            if api_provider == "OpenAI":
                stream = client.chat.completions.create(
                    model=selected_model,
                    messages=[
                        {"role": "system", "content": custom_prompt},
                        {"role": "user", "content": f"ê³µê³ ë‚´ìš©: {pdf_text}\n\níšŒì‚¬ì •ë³´: {company_info}"}
                    ],
                    stream=True
                )
                
                for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        generated_text += chunk.choices[0].delta.content
                        placeholder.markdown(generated_text + "â–Œ")
                
                proposal = generated_text
                
            else:
                message = claude.messages.create(
                    model=selected_model,
                    max_tokens=4000,
                    messages=[
                        {
                            "role": "user",
                            "content": f"{custom_prompt}\n\nê³µê³ ë‚´ìš©: {pdf_text}\n\níšŒì‚¬ì •ë³´: {company_info}"
                        }
                    ]
                )
                generated_text = message.content[0].text
                placeholder.markdown(generated_text)
                proposal = generated_text
                
            placeholder.markdown(generated_text)
            st.session_state.proposal_chunks = split_by_headers(proposal)

# ì²­í¬ë³„ í‘œì‹œ ë° í”¼ë“œë°±
if st.session_state.proposal_chunks:
    for i, chunk in enumerate(st.session_state.proposal_chunks):
        with st.expander(f"íŒŒíŠ¸ {i+1}", expanded=True):
            st.markdown(chunk)
            feedback = st.text_area(f"íŒŒíŠ¸ {i+1}ì— ëŒ€í•œ í”¼ë“œë°±", key=f"feedback_{i}")
            
            if st.button(f"íŒŒíŠ¸ {i+1} ìˆ˜ì •", key=f"revise_{i}"):
                with st.spinner("ìˆ˜ì • ì¤‘..."):
                    placeholder = st.empty()
                    generated_text = ""
                    
                    if api_provider == "OpenAI":
                        stream = client.chat.completions.create(
                            model=selected_model,
                            messages=[
                                {"role": "system", "content": custom_prompt},
                                {"role": "user", "content": f"ì›ë³¸ ë‚´ìš©:\n{chunk}\n\ní”¼ë“œë°±:\n{feedback}\n\nì´ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ë‚´ìš©ì„ ìˆ˜ì •í•´ì£¼ì„¸ìš”."}
                            ],
                            stream=True
                        )
                        
                        for chunk in stream:
                            if chunk.choices[0].delta.content is not None:
                                generated_text += chunk.choices[0].delta.content
                                placeholder.markdown(generated_text + "â–Œ")
                                
                        revised_chunk = generated_text
                        
                    else:
                        message = claude.messages.create(
                            model=selected_model,
                            max_tokens=4000,
                            messages=[
                                {
                                    "role": "user",
                                    "content": f"{custom_prompt}\n\nì›ë³¸ ë‚´ìš©:\n{chunk}\n\ní”¼ë“œë°±:\n{feedback}\n\nì´ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ë‚´ìš©ì„ ìˆ˜ì •í•´ì£¼ì„¸ìš”."
                                }
                            ]
                        )
                        generated_text = message.content[0].text
                        placeholder.markdown(generated_text)
                        revised_chunk = generated_text
                    
                    placeholder.markdown(generated_text)
                    st.session_state.proposal_chunks[i] = revised_chunk
                    st.rerun()

if st.session_state.proposal_chunks:
    # ì „ì²´ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
    full_proposal = "\n\n".join(st.session_state.proposal_chunks)
    st.download_button(
        "ì „ì²´ ì‚¬ì—…ê³„íšì„œ ë‹¤ìš´ë¡œë“œ",
        full_proposal,
        "proposal.md",
        "text/markdown"
    )
